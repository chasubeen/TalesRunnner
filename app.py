### === 0. Import Libraries ===
import streamlit as st 

import os
import torch
import textwrap
import hashlib
import random
import io
import tempfile
import uuid

from deep_translator import GoogleTranslator 
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM
from openai import OpenAI

from reportlab.pdfgen import canvas
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib.utils import ImageReader


### === 1. Configs ===
Image.MAX_IMAGE_PIXELS = None

FONT_PATH = os.getenv("FONT_PATH", "/usr/share/fonts/truetype/nanum/NanumGothic.ttf") 
pdfmetrics.registerFont(TTFont("NanumGothic", FONT_PATH))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "YOUR_DEFAULT_API_KEY")
client = OpenAI(api_key = OPENAI_API_KEY)

## Hugging Face ëª¨ë¸ ë¡œë“œ 
@st.cache_resource
def load_models():
    """
    Hugging Face ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
    - ìºì‹±ì„ í™œìš©í•˜ì—¬ ë¡œë”© ì†ë„ ì¦ê°€
    """
    # Image Captioning
    blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
    
    # Story Generator
    MODEL_DIR = os.getenv("MODEL_DIR", "YOUR_MODEL_DIR")  # Fine-tuned koT5 

    required_files = ["config.json", "model.safetensors", "tokenizer.json"]
    missing_files = [f for f in required_files if not os.path.exists(os.path.join(MODEL_DIR, f))]
    if not os.path.exists(MODEL_DIR) or missing_files:
        raise ValueError(f"ğŸš¨ ëª¨ë¸ ê²½ë¡œê°€ ì˜¬ë°”ë¥´ì§€ ì•Šê±°ë‚˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {MODEL_DIR}\nâŒ ëˆ„ë½ëœ íŒŒì¼: {missing_files}")

    t5_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)
    t5_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR, local_files_only=True).to(device)
    
    return blip_processor, blip_model, t5_tokenizer, t5_model

blip_processor, blip_model, t5_tokenizer, t5_model = load_models()


### === 2. utils ===
@st.cache_data
def save_temp_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ ì €ì¥"""
    temp_dir = "/content/temp_images"
    os.makedirs(temp_dir, exist_ok=True) 
    
    file_ext = os.path.splitext(uploaded_file.name)[-1] 
    temp_path = os.path.join(temp_dir, f"{uuid.uuid4().hex}{file_ext}")

    with open(temp_path, "wb") as f:
        f.write(uploaded_file.getvalue())
    return temp_path

@st.cache_data
def translate_text(text, src="en", dest="ko"):
    """ë¬¸ìì—´ì„ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜ (deep-translator ì‚¬ìš©)"""
    try:
        return GoogleTranslator(source=src, target=dest).translate(text)
    except Exception as e:
        return text  

## ğŸ“· ì´ë¯¸ì§€ ìº¡ì…”ë‹ (BLIP ëª¨ë¸ í™œìš©)
@st.cache_data
def generate_korean_caption(image_path):
    """
    ì´ë¯¸ì§€ì—ì„œ ìº¡ì…˜ ìƒì„± í›„ í•œêµ­ì–´ë¡œ ë²ˆì—­ (googletrans ì‚¬ìš©)
    """
    try:
        image = Image.open(image_path).convert('RGB')
        inputs = blip_processor(image, return_tensors="pt").to(device)
        outputs = blip_model.generate(**inputs)
        english_caption = blip_processor.decode(outputs[0], skip_special_tokens=True)
        korean_caption = translate_text(english_caption, src="en", dest="ko")
        image.close()
        return korean_caption
    except Exception as e:
        st.warning(f"ğŸš¨ ë²ˆì—­ ì‹¤íŒ¨: {e}, ì›ë³¸ ì˜ì–´ ìº¡ì…˜ ë°˜í™˜.")
        return english_caption


## ğŸ­ ë³´ì¡° ì •ë³´ ìë™ ì¶”ì²œ
@st.cache_data
def suggest_story_elements(caption):
    """
    GPT-4 APIë¥¼ í™œìš©í•˜ì—¬ ë³´ì¡°ì  ì •ë³´ ì¶”ì²œ
    """
    prompt = (
        f"ë‹¤ìŒ ì´ë¯¸ì§€ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ì•¼ê¸°ì— ì í•©í•œ ë°°ê²½, í–‰ë™, ê°ì • ìš”ì†Œë¥¼ ì¶”ì²œí•´ì¤˜.\n"
        f"ê° ìš”ì†ŒëŠ” 3~5ê°œì”© ì¶”ì²œí•˜ê³ , ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ í˜•íƒœë¡œë§Œ ì¶œë ¥í•´.\n"
        f"\nì„¤ëª…: {caption}\n"
        f"\nì¶œë ¥ ì˜ˆì‹œ (ë°°ê²½: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3 / í–‰ë™: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2 / ê°ì •: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3):"
    )
    response = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì´ì•¼ê¸° ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        model="gpt-4o",
        max_tokens=50
    )
    return response.choices[0].message.content


## input text ìƒì„± 
@st.cache_data
def generate_input_text(caption, name, i_action, classification, additional_info):
    seed_source = f"{caption}{name}{i_action}"
    seed_value = int(hashlib.md5(seed_source.encode()).hexdigest(), 16) % (10 ** 8)
    random.seed(seed_value) 

    required_fields = [
        f"<caption> {caption}",
        f"<name> {name.strip()}",
        f"<i_action> {i_action.strip()}",
        f"<classification> {classification.strip()}"
    ]

    optional_tokens = [
        f"{token} {additional_info.get(field, '<empty>') if additional_info else '<empty>'}"
        for field, token in {
            "character": "<character>",
            "setting": "<setting>",
            "action": "<action>",
            "feeling": "<feeling>",
            "causalRelationship": "<causalRelationship>",
            "outcomeResolution": "<outcomeResolution>",
            "prediction": "<prediction>"
        }.items()
    ]

    all_tokens = required_fields + optional_tokens
    random.shuffle(all_tokens)

    return "generate: " + " ".join(all_tokens)


## ğŸ“ ìŠ¤í† ë¦¬ ìƒì„±
special_tokens = [
    "<caption>", "<name>", "<i_action>", "<classification>", "<character>",
    "<setting>", "<action>", "<feeling>", "<causalRelationship>",
    "<outcomeResolution>", "<prediction>", "<empty>"
]
special_token_ids = t5_tokenizer.convert_tokens_to_ids(special_tokens)

def special_token_masking(input_ids, attention_mask, special_token_ids):
    special_token_ids = torch.tensor(special_token_ids, device=input_ids.device)
    mask_indices = (input_ids.unsqueeze(-1) == torch.tensor(special_token_ids).clone().detach()).any(dim=-1)
    attention_mask[mask_indices] = 0
    return attention_mask

@st.cache_data
def generate_story(input_text):
    inputs = t5_tokenizer(input_text, max_length=128, padding="max_length", truncation=True, return_tensors="pt").to(device)
    inputs["attention_mask"] = special_token_masking(inputs["input_ids"], inputs["attention_mask"], special_token_ids).to(device)
    
    with torch.no_grad():
        output_ids = t5_model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"], 
            max_length=128,
            num_beams=3,
            length_penalty=0.8,
            repetition_penalty=1.5,
            no_repeat_ngram_size=3,
            early_stopping=True
        )
    return t5_tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)


@st.cache_data
def save_story_as_pdf(story_data):
    """ì´ì•¼ê¸°ë¥¼ PDFë¡œ ì €ì¥ (í•œê¸€ ê¹¨ì§ ë°©ì§€)"""
    pdf_buffer = io.BytesIO()
    c = canvas.Canvas(pdf_buffer)
    
    c.setFont("NanumGothic", 12)

    for row in story_data:
        wrapped_text = textwrap.wrap(row["story"], width=40) 
        y_position = 750
        image_path = row.get("imgpath")

        if not image_path or not os.path.exists(image_path):
            image_path = "/content/default_image.jpg"
            if not os.path.exists(image_path):
                raise ValueError(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ: ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤! â†’ {image_path}")

        try:
            img = ImageReader(image_path)
            c.drawImage(img, 50, 500, width=400, height=300, preserveAspectRatio=True)
            y_position = 480
        except Exception as e:
            raise RuntimeError(f"ğŸš¨ ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜ ë°œìƒ: {e}")

        for line in wrapped_text:
            if y_position < 50:
                c.showPage()
                c.setFont("NanumGothic", 12) 
                y_position = 750
            c.drawString(50, y_position, line)
            y_position -= 20

        c.showPage()
        c.setFont("NanumGothic", 12)

    c.save()
    pdf_buffer.seek(0)
    return pdf_buffer

    
    
### === 3. Streamlit UI êµ¬ì„± ===
st.title("ğŸ“– AI ë™í™”ì±… ìƒì„±ê¸°")
st.write("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´ AIê°€ ìë™ìœ¼ë¡œ ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤ âœ¨")

'''
- ì‚¬ìš©ì ì´ë¯¸ì§€ ì—…ë¡œë“œ
- captioning ìˆ˜í–‰
- ë³´ì¡° ì •ë³´ ì¶”ì²œ
'''
uploaded_files = st.file_uploader("ğŸ“¸ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”(ìµœëŒ€ 10ì¥)", type=["jpg", "png", "jpeg"], accept_multiple_files=True)

if uploaded_files:
    user_inputs = []

    if "uploaded_image_paths" not in st.session_state:
        st.session_state["uploaded_image_paths"] = {}

    for idx, uploaded_file in enumerate(uploaded_files):
        img_path = save_temp_file(uploaded_file) 

        st.session_state["uploaded_image_paths"][uploaded_file.name] = img_path

        image = Image.open(img_path).convert("RGB")
        st.image(image, caption=f"ì—…ë¡œë“œëœ ì´ë¯¸ì§€{idx + 1}", use_container_width=True)
        
        with st.spinner("ğŸ“¸ ìº¡ì…˜ ìƒì„± ì¤‘..."):
            caption = generate_korean_caption(img_path)       
        st.write(f"ğŸ“Œ **ìë™ ìƒì„±ëœ ìº¡ì…˜:** {caption}")
        
        st.subheader(f"ğŸ’¬ ë³´ì¡° ì •ë³´ ì…ë ¥{idx + 1}")
        
        name = st.text_input("ğŸ“ ê°ì²´ ì´ë¦„", placeholder="ex) ê°•ì•„ì§€", key=f"name_{idx + 1}")
        i_action = st.text_input("ğŸ­ ê°ì²´ì˜ í–‰ë™", placeholder="ex) ë›°ë‹¤", key=f"i_action_{idx + 1}")
        classification = st.selectbox("ğŸ“š ì´ì•¼ê¸° ë¶„ë¥˜", ["ì˜ì‚¬ì†Œí†µ", "ìì—°íƒêµ¬", "ì‚¬íšŒê´€ê³„", "ì˜ˆìˆ ê²½í—˜", "ì‹ ì²´ìš´ë™_ê±´ê°•"], key=f"classification_{idx + 1}")
        
        suggested_info = suggest_story_elements(caption)
        st.write(f"ğŸ”¹ ì¶”ì²œ ì •ë³´({idx + 1}): {suggested_info}")
        
        additional_info = {
            "character": st.text_input("ğŸ‘¤ ìºë¦­í„° ì •ë³´ (ì„ íƒ)", placeholder="ex) ì£¼ì¸ê³µ, ì¹œêµ¬", key=f"character_{idx + 1}"),
            "setting": st.text_input("ğŸï¸ ë°°ê²½ ì •ë³´ (ì„ íƒ)", placeholder="ex) ìˆ²ì†, ë³‘ì›", key=f"setting_{idx + 1}"),
            "action": st.text_input("âš¡ í–‰ë™ ì„¤ëª… (ì„ íƒ)", placeholder="ex) ë†€ë€ë‹¤, ê³ ë¯¼í•˜ë‹¤", key=f"action_{idx + 1}"),
            "feeling": st.text_input("ğŸ˜ƒ ê°ì • ìƒíƒœ (ì„ íƒ)", placeholder="ex) ê¸°ë»í•˜ë‹¤, ë†€ë¼ë‹¤", key=f"feeling_{idx + 1}"),
            "causalRelationship": st.text_input("ğŸ”— ì¸ê³¼ ê´€ê³„ (ì„ íƒ)", placeholder="ex) ì¹œêµ¬ë¥¼ ë§Œë‚¬ë‹¤ â†’ í•¨ê»˜ ëª¨í—˜ì„ ë– ë‚¬ë‹¤", key=f"causalRelationship_{idx + 1}"),
            "outcomeResolution": st.text_input("ğŸ¯ ê²°ê³¼ (ì„ íƒ)", placeholder="ex) ì˜¨ ê°€ì¡±ì´ ê¸°ë»í–ˆë‹¤", key=f"outcomeResolution_{idx + 1}"),
            "prediction": st.text_input("ğŸ”® ì˜ˆì¸¡ (ì„ íƒ)", placeholder="ex) ì´í›„ ì™•êµ­ì„ êµ¬í•œë‹¤", key=f"prediction_{idx + 1}")
        }

        user_inputs.append({
            "imgpath": img_path,
            "caption": caption,
            "name": name,
            "i_action": i_action,
            "classification": classification,
            "additional_info": additional_info
        })

    if st.button("ğŸš€ ì´ì•¼ê¸° ìƒì„±"):
        stories = []
        
        for user_input in user_inputs:
            input_text = generate_input_text(
                user_input["caption"], 
                user_input["name"], 
                user_input["i_action"], 
                user_input["classification"], 
                user_input["additional_info"]
            )
            story = generate_story(input_text) 
            stories.append({"imgpath": user_input["imgpath"], "story": story})

        st.success("ì´ì•¼ê¸°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! âœ…")

        pdf_buffer = save_story_as_pdf(stories)
        st.download_button("ğŸ“¥ PDFë¡œ ì €ì¥", data=pdf_buffer, file_name="storybook.pdf", mime="application/pdf")